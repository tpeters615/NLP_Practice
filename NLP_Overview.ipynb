{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "### Part 1: NLP intro \n",
    "\n",
    "##### NLP is the task we give computers to read and understand (process) written text (natural language). By far, the most popular toolkit or API to do natural language processing is the Natural Language Toolkit for the Python programming language. \n",
    "\n",
    "##### The NLTK module comes packed full of everything from trained algorithms to identify parts of speech to unsupervised machine learning algorithms to help you train your own machine to understand a specific bit of text. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Great site for python documentation/tutorials/help:\n",
    "https://pythonprogramming.net/\n",
    "##### Walkthrough/tutorial from Sentdex on Youtube\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/75/ce35194d8e3022203cca0d2f896dbb88689f9b3fce8e9f9cff942913519d/nltk-3.5.zip (1.4MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4MB 3.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting click (from nltk)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/3d/fa76db83bf75c4f8d338c2fd15c8d33fdd7ad23a9b5e57eb6c5de26b430e/click-7.1.2-py2.py3-none-any.whl (82kB)\n",
      "\u001b[K     |████████████████████████████████| 92kB 25.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting joblib (from nltk)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b8/a6/d1a816b89aa1e9e96bcb298eb1ee1854f21662ebc6d55ffa3d7b3b50122b/joblib-0.15.1-py3-none-any.whl (298kB)\n",
      "\u001b[K     |████████████████████████████████| 307kB 25.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex (from nltk)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/7c/0d46b10a87b3087e8e303fac923beb19ec839d7c5ea34971a12fafb22b52/regex-2020.5.14-cp36-cp36m-manylinux2010_x86_64.whl (675kB)\n",
      "\u001b[K     |████████████████████████████████| 686kB 8.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm (from nltk)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c9/40/058b12e8ba10e35f89c9b1fdfc2d4c7f8c05947df2d5eb3c7b258019fda0/tqdm-4.46.0-py2.py3-none-any.whl (63kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 6.4MB/s  eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jupyterlab/.cache/pip/wheels/ae/8c/3f/b1fe0ba04555b08b57ab52ab7f86023639a526d8bc8d384306\n",
      "Successfully built nltk\n",
      "Installing collected packages: click, joblib, regex, tqdm, nltk\n",
      "Successfully installed click-7.1.2 joblib-0.15.1 nltk-3.5 regex-2020.5.14 tqdm-4.46.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Downloader>  L\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Packages:\n",
      "  [ ] abc................. Australian Broadcasting Commission 2006\n",
      "  [ ] alpino.............. Alpino Dutch Treebank\n",
      "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
      "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
      "  [ ] basque_grammars..... Grammars for Basque\n",
      "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
      "                           Extraction Systems in Biology)\n",
      "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
      "  [ ] book_grammars....... Grammars from NLTK Book\n",
      "  [ ] brown............... Brown Corpus\n",
      "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
      "  [ ] cess_cat............ CESS-CAT Treebank\n",
      "  [ ] cess_esp............ CESS-ESP Treebank\n",
      "  [ ] chat80.............. Chat-80 Data Files\n",
      "  [ ] city_database....... City Database\n",
      "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
      "  [ ] comparative_sentences Comparative Sentence Dataset\n",
      "  [ ] comtrans............ ComTrans Corpus Sample\n",
      "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
      "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Hit Enter to continue:  q\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Downloader>  d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Download which package (l=list; x=cancel)?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  Identifier>  all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    Downloading collection 'all'\n",
      "       | \n",
      "       | Downloading package abc to /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/abc.zip.\n",
      "       | Downloading package alpino to /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/alpino.zip.\n",
      "       | Downloading package biocreative_ppi to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/biocreative_ppi.zip.\n",
      "       | Downloading package brown to /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/brown.zip.\n",
      "       | Downloading package brown_tei to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/brown_tei.zip.\n",
      "       | Downloading package cess_cat to /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/cess_cat.zip.\n",
      "       | Downloading package cess_esp to /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/cess_esp.zip.\n",
      "       | Downloading package chat80 to /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/chat80.zip.\n",
      "       | Downloading package city_database to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/city_database.zip.\n",
      "       | Downloading package cmudict to /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/cmudict.zip.\n",
      "       | Downloading package comparative_sentences to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/comparative_sentences.zip.\n",
      "       | Downloading package comtrans to /home/jupyterlab/nltk_data...\n",
      "       | Downloading package conll2000 to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/conll2000.zip.\n",
      "       | Downloading package conll2002 to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/conll2002.zip.\n",
      "       | Downloading package conll2007 to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       | Downloading package crubadan to /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/crubadan.zip.\n",
      "       | Downloading package dependency_treebank to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/dependency_treebank.zip.\n",
      "       | Downloading package dolch to /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/dolch.zip.\n",
      "       | Downloading package europarl_raw to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/europarl_raw.zip.\n",
      "       | Downloading package floresta to /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/floresta.zip.\n",
      "       | Downloading package framenet_v15 to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/framenet_v15.zip.\n",
      "       | Downloading package framenet_v17 to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/framenet_v17.zip.\n",
      "       | Downloading package gazetteers to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/gazetteers.zip.\n",
      "       | Downloading package genesis to /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/genesis.zip.\n",
      "       | Downloading package gutenberg to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/gutenberg.zip.\n",
      "       | Downloading package ieer to /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/ieer.zip.\n",
      "       | Downloading package inaugural to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/inaugural.zip.\n",
      "       | Downloading package indian to /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/indian.zip.\n",
      "       | Downloading package jeita to /home/jupyterlab/nltk_data...\n",
      "       | Downloading package kimmo to /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/kimmo.zip.\n",
      "       | Downloading package knbc to /home/jupyterlab/nltk_data...\n",
      "       | Downloading package lin_thesaurus to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/lin_thesaurus.zip.\n",
      "       | Downloading package mac_morpho to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/mac_morpho.zip.\n",
      "       | Downloading package machado to /home/jupyterlab/nltk_data...\n",
      "       | Downloading package masc_tagged to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       | Downloading package moses_sample to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping models/moses_sample.zip.\n",
      "       | Downloading package movie_reviews to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/movie_reviews.zip.\n",
      "       | Downloading package names to /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/names.zip.\n",
      "       | Downloading package nombank.1.0 to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       | Downloading package nps_chat to /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/nps_chat.zip.\n",
      "       | Downloading package omw to /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/omw.zip.\n",
      "       | Downloading package opinion_lexicon to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/opinion_lexicon.zip.\n",
      "       | Downloading package paradigms to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/paradigms.zip.\n",
      "       | Downloading package pil to /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/pil.zip.\n",
      "       | Downloading package pl196x to /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/pl196x.zip.\n",
      "       | Downloading package ppattach to /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/ppattach.zip.\n",
      "       | Downloading package problem_reports to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/problem_reports.zip.\n",
      "       | Downloading package propbank to /home/jupyterlab/nltk_data...\n",
      "       | Downloading package ptb to /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/ptb.zip.\n",
      "       | Downloading package product_reviews_1 to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/product_reviews_1.zip.\n",
      "       | Downloading package product_reviews_2 to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/product_reviews_2.zip.\n",
      "       | Downloading package pros_cons to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/pros_cons.zip.\n",
      "       | Downloading package qc to /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/qc.zip.\n",
      "       | Downloading package reuters to /home/jupyterlab/nltk_data...\n",
      "       | Downloading package rte to /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/rte.zip.\n",
      "       | Downloading package semcor to /home/jupyterlab/nltk_data...\n",
      "       | Downloading package senseval to /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/senseval.zip.\n",
      "       | Downloading package sentiwordnet to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/sentiwordnet.zip.\n",
      "       | Downloading package sentence_polarity to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/sentence_polarity.zip.\n",
      "       | Downloading package shakespeare to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/shakespeare.zip.\n",
      "       | Downloading package sinica_treebank to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/sinica_treebank.zip.\n",
      "       | Downloading package smultron to /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/smultron.zip.\n",
      "       | Downloading package state_union to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/state_union.zip.\n",
      "       | Downloading package stopwords to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/stopwords.zip.\n",
      "       | Downloading package subjectivity to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/subjectivity.zip.\n",
      "       | Downloading package swadesh to /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/swadesh.zip.\n",
      "       | Downloading package switchboard to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/switchboard.zip.\n",
      "       | Downloading package timit to /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/timit.zip.\n",
      "       | Downloading package toolbox to /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/toolbox.zip.\n",
      "       | Downloading package treebank to /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/treebank.zip.\n",
      "       | Downloading package twitter_samples to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/twitter_samples.zip.\n",
      "       | Downloading package udhr to /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/udhr.zip.\n",
      "       | Downloading package udhr2 to /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/udhr2.zip.\n",
      "       | Downloading package unicode_samples to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/unicode_samples.zip.\n",
      "       | Downloading package universal_treebanks_v20 to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       | Downloading package verbnet to /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/verbnet.zip.\n",
      "       | Downloading package verbnet3 to /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/verbnet3.zip.\n",
      "       | Downloading package webtext to /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/webtext.zip.\n",
      "       | Downloading package wordnet to /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/wordnet.zip.\n",
      "       | Downloading package wordnet_ic to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/wordnet_ic.zip.\n",
      "       | Downloading package words to /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/words.zip.\n",
      "       | Downloading package ycoe to /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/ycoe.zip.\n",
      "       | Downloading package rslp to /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping stemmers/rslp.zip.\n",
      "       | Downloading package maxent_treebank_pos_tagger to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
      "       | Downloading package universal_tagset to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping taggers/universal_tagset.zip.\n",
      "       | Downloading package maxent_ne_chunker to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "       | Downloading package punkt to /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping tokenizers/punkt.zip.\n",
      "       | Downloading package book_grammars to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping grammars/book_grammars.zip.\n",
      "       | Downloading package sample_grammars to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping grammars/sample_grammars.zip.\n",
      "       | Downloading package spanish_grammars to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping grammars/spanish_grammars.zip.\n",
      "       | Downloading package basque_grammars to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping grammars/basque_grammars.zip.\n",
      "       | Downloading package large_grammars to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping grammars/large_grammars.zip.\n",
      "       | Downloading package tagsets to /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping help/tagsets.zip.\n",
      "       | Downloading package snowball_data to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       | Downloading package bllip_wsj_no_aux to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping models/bllip_wsj_no_aux.zip.\n",
      "       | Downloading package word2vec_sample to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping models/word2vec_sample.zip.\n",
      "       | Downloading package panlex_swadesh to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       | Downloading package mte_teip5 to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/mte_teip5.zip.\n",
      "       | Downloading package averaged_perceptron_tagger to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "       | Downloading package averaged_perceptron_tagger_ru to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping taggers/averaged_perceptron_tagger_ru.zip.\n",
      "       | Downloading package perluniprops to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping misc/perluniprops.zip.\n",
      "       | Downloading package nonbreaking_prefixes to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
      "       | Downloading package vader_lexicon to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       | Downloading package porter_test to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping stemmers/porter_test.zip.\n",
      "       | Downloading package wmt15_eval to\n",
      "       |     /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping models/wmt15_eval.zip.\n",
      "       | Downloading package mwa_ppdb to /home/jupyterlab/nltk_data...\n",
      "       |   Unzipping misc/mwa_ppdb.zip.\n",
      "       | \n",
      "     Done downloading collection all\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Downloader>  q\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 continued\n",
    "\n",
    "##### Word and sentence tokenizers: break up by word or sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word and sentence tokenizers: break up by word or sentence\n",
    "# lexicoon and corporas\n",
    "# corpora: body of text \n",
    "# lexicon: dictionary (words and their meanings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word and sent tokenizers \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Mr. Smith, how are you doing today?', 'The weather is great and Python is awesome.']\n",
      "['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', 'and', 'Python', 'is', 'awesome', '.']\n"
     ]
    }
   ],
   "source": [
    "example_text = 'Hello Mr. Smith, how are you doing today? The weather is great and Python is awesome.'\n",
    "print(sent_tokenize(example_text))\n",
    "print(word_tokenize(example_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Stopwords: garbage words (i.e. not informative) in english language \n",
    "\n",
    "##### One of the largest elements to any data analysis, natural language processing included, is pre-processing. This is the methodology used to \"clean up\" and prepare your data for analysis. \n",
    "\n",
    "##### One of the first steps to pre-processing is to utilize stop-words. Stop words are words that you want to filter out of any analysis. These are words that carry no meaning, or carry conflicting meanings that you simply do not want to deal with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'example', 'showing', 'stop', 'word', 'filtration', '.']\n",
      "['This', 'example', 'showing', 'stop', 'word', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "# Stopwords: garbage words in english language \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "ex = 'This is an example showing off stop word filtration.'\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered_list = []\n",
    "word_toke = word_tokenize(ex)\n",
    "for word in word_toke:\n",
    "    if word not in stop_words:\n",
    "        filtered_list.append(word)\n",
    "print(filtered_list)\n",
    "# one liner\n",
    "filtered_sentence = [w for w in word_toke if w not in stop_words]\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Stemming\n",
    "\n",
    "##### Another form of data pre-processing with natural language processing is called \"stemming.\" This is the process where we remove word affixes from the end of words. The reason we would do this is so that we do not need to store the meaning of every single tense of a word. For example:\n",
    "\n",
    "##### Reader, Reading, Read\n",
    "\n",
    "##### Aside from tense, and even one of these is a noun, they all have the same meaning for their \"root\" stem (read).\n",
    "\n",
    "##### This way, we store one single value for the root stem of \"read.\" Then, when we wish to learn more, we can look into the affixes that were on the end, like \"ing\" is an active word, or in the past, then you have reader as someone who reads... then just plain read as either past tense or current. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "pythonli\n"
     ]
    }
   ],
   "source": [
    "## stemming \n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "ps = PorterStemmer()\n",
    "ex = ['python','pythoner', 'pythoning', 'pythoned', 'pythonly']\n",
    "\n",
    "for w in ex:\n",
    "    print(ps.stem(w))\n",
    "new_text = 'It is very important to be pythonly while you are pythoning with python. All pythoners have pythoned poorly at least once.'\n",
    "\n",
    "words = word_tokenize(new_text)\n",
    "#for w in words: \n",
    "     #print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Part of Speech Tagging \n",
    "\n",
    "##### Part of Speech tagging does exactly what it sounds like, it tags each word in a sentence with the part of speech for that word. This means it labels words as noun, adjective, verb, etc. PoS tagging also covers tenses of the parts of speech. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part of speech tagging \n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer \n",
    "\n",
    "train_text = state_union.raw('2005-GWBush.txt')\n",
    "sample_text = state_union.raw('2006-GWBush.txt')\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try: \n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            print(tagged)\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "#process_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 6: Chunking \n",
    "\n",
    "##### Chunking in Natural Language Processing (NLP) is the process by which we group various words together by their part of speech tags. \n",
    "\n",
    "##### One of the most popular uses of this is to group things by what are called \"noun phrases.\" We do this to find the main subjects and descriptive words around them, but chunking can be used for any combination of parts of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chunking: grouping words into phrases\n",
    "\n",
    "train_text = state_union.raw('2005-GWBush.txt')\n",
    "sample_text = state_union.raw('2006-GWBush.txt')\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try: \n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            # reg expression doc :https://pythonprogramming.net/regular-expressions-regex-tutorial-python-3/\n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"  \n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            #print(chunked)\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "process_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 7: Chinking\n",
    "\n",
    "##### Chinking is a part of the chunking process with natural language processing with NLTK. A chink is what we wish to remove from the chunk. We define a chink in a very similar fashion compared to how we defined the chunk. \n",
    "\n",
    "##### The reason why you may want to use a chink is when your chunker is getting almost everything you want, but is also picking up some things you don't want. You could keep adding chunker rules, but it may be far easier to just specify a chink to remove from the chunk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Chinking is a part of the chunking process with natural language processing with NLTK. \n",
    "\n",
    "def process_content():\n",
    "    try: \n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            # reg expression doc :https://pythonprogramming.net/regular-expressions-regex-tutorial-python-3/\n",
    "            chunkGram = r\"\"\"Chunk: {<.*>+} \n",
    "                                    }<VB.?|IN|DT>+{\"\"\"  \n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            #print(chunked)\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "process_content()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 8: Named Entity \n",
    "\n",
    "##### Named entity recognition is useful to quickly find out what the subjects of discussion are. NLTK comes packed full of options for us. We can find just about any named entity, or we can look for specific ones.\n",
    "\n",
    "##### NLTK can either recognize a general named entity, or it can even recognize locations, names, monetary amounts, dates, and more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named Entity Recognition \n",
    "\n",
    "def process_content():\n",
    "    try: \n",
    "        for i in tokenized[5:]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            namedEnt = nltk.ne_chunk(tagged, binary = True)\n",
    "    \n",
    "            #print(namedEnt)\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "process_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 9: Lemmatizing \n",
    "\n",
    "##### A very similar operation to stemming is called lemmatizing. The major difference between these is, as you saw earlier, stemming can often create non-existent words.\n",
    "\n",
    "##### So, your root stem, meaning the word you end up with, is not something you can just look up in a dictionary.\n",
    "\n",
    "##### A root lemma, on the other hand, is a real word. Many times, you will wind up with a very similar word, but sometimes, you will wind up with a completely different word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n",
      "best\n",
      "run\n",
      "interest\n"
     ]
    }
   ],
   "source": [
    "# Lemmatizing: finding root stems of original words in text data\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "print(lemmatizer.lemmatize(\"better\", pos='a'))\n",
    "print(lemmatizer.lemmatize(\"best\", pos='a'))\n",
    "print(lemmatizer.lemmatize(\"ran\", pos='v'))\n",
    "print(lemmatizer.lemmatize(\"interested\", pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 10: Corpora \n",
    "\n",
    "##### Remember from the beginning, we talked about this term, \"corpora.\"\n",
    "\n",
    "##### Again, corpora is just a body of texts. Generally, corpora are grouped by some sort of defining characteristic.\n",
    "\n",
    "##### NLTK is a massive toolkit for you. part of what they give you is a ton of highly valuable corpora to learn with, train against, and some of them are even capable of using in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyterlab/conda/envs/python/lib/python3.6/site-packages/nltk/__init__.py\n",
      "['1:5 And God called the light Day, and the darkness he called Night.', 'And the evening and the morning were the first day.', '1:6 And God said, Let there be a firmament in the midst of the waters,\\nand let it divide the waters from the waters.', '1:7 And God made the firmament, and divided the waters which were\\nunder the firmament from the waters which were above the firmament:\\nand it was so.', '1:8 And God called the firmament Heaven.', 'And the evening and the\\nmorning were the second day.', '1:9 And God said, Let the waters under the heaven be gathered together\\nunto one place, and let the dry land appear: and it was so.', '1:10 And God called the dry land Earth; and the gathering together of\\nthe waters called he Seas: and God saw that it was good.', '1:11 And God said, Let the earth bring forth grass, the herb yielding\\nseed, and the fruit tree yielding fruit after his kind, whose seed is\\nin itself, upon the earth: and it was so.', '1:12 And the earth brought forth grass, and herb yielding seed after\\nhis kind, and the tree yielding fruit, whose seed was in itself, after\\nhis kind: and God saw that it was good.']\n"
     ]
    }
   ],
   "source": [
    "# Corpora: accessing amd viewing \n",
    "\n",
    "print(nltk.__file__)\n",
    "\n",
    "from nltk.corpus import gutenberg \n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sample = gutenberg.raw('bible-kjv.txt')\n",
    "sentence = sent_tokenize(sample)\n",
    "print(sentence[5:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 11: WordNet\n",
    "\n",
    "##### Part of the NLTK Corpora is WordNet. I wouldn't totally classify WordNet as a Corpora, if anything it is really a giant Lexicon, but, either way, it is super useful. With WordNet we can do things like look up words and their meaning according to their parts of speech, we can find synonyms, antonyms, and even examples of the word in use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plan.n.01\n",
      "plan\n",
      "a series of steps to be carried out or goals to be accomplished\n",
      "['they drew up a six-step plan', 'they discussed plans for a new bond issue']\n",
      "{'sound', 'unspoilt', 'honest', 'adept', 'salutary', 'goodness', 'undecomposed', 'dependable', 'just', 'secure', 'safe', 'serious', 'upright', 'dear', 'skilful', 'honorable', 'practiced', 'full', 'trade_good', 'right', 'ripe', 'in_force', 'good', 'thoroughly', 'soundly', 'well', 'expert', 'proficient', 'respectable', 'estimable', 'near', 'unspoiled', 'commodity', 'skillful', 'effective', 'in_effect', 'beneficial'}\n",
      "{'ill', 'evil', 'badness', 'bad', 'evilness'}\n"
     ]
    }
   ],
   "source": [
    "# WordNet \n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "### finding synonyms, defintion and examples\n",
    "## ex = program\n",
    "syns = wordnet.synsets('program') \n",
    "print(syns[0].name()) \n",
    "# lemma gets root word (similar to lemmatizer)\n",
    "print(syns[0].lemmas()[0].name()) # word\n",
    "print(syns[0].definition()) # definition\n",
    "print(syns[0].examples()) # examples\n",
    "\n",
    "# finding antonyms \n",
    "synonyms = []\n",
    "antonyms = []\n",
    "\n",
    "\n",
    "# finding synonyms and antonyms \n",
    "for syn in wordnet.synsets('good'):\n",
    "    # since using lemma, gonna give synonyms for root word\n",
    "    # good could mean an object or an adjective etc. \n",
    "    for l in syn.lemmas(): \n",
    "        #print(\"l:\",l) # looking at the different lemmas \n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "            \n",
    "print(set(synonyms))\n",
    "print(set(antonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9090909090909091\n",
      "0.6956521739130435\n",
      "0.32\n"
     ]
    }
   ],
   "source": [
    "# wordnet continued \n",
    "\n",
    "# how similar are two words \n",
    "w1 = wordnet.synset('ship.n.01')\n",
    "w2 = wordnet.synset('boat.n.01')\n",
    "print(w1.wup_similarity(w2))\n",
    "\n",
    "# how similar are two words \n",
    "w1 = wordnet.synset('ship.n.01')\n",
    "w2 = wordnet.synset('car.n.01')\n",
    "print(w1.wup_similarity(w2))\n",
    "\n",
    "# how similar are two words \n",
    "w1 = wordnet.synset('ship.n.01')\n",
    "w2 = wordnet.synset('cat.n.01')\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 11: Binary Text Classification \n",
    "\n",
    "##### Now that we understand some of the basics of of natural language processing with the Python NLTK module, we're ready to try out text classification. This is where we attempt to identify a body of text with some sort of label. \n",
    "\n",
    "##### To start, we're going to use some sort of binary label. Examples of this could be identifying text as spam or not, or, like what we'll be doing, positive sentiment or negative sentiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg\n",
      "pos\n"
     ]
    }
   ],
   "source": [
    "# Text Classification \n",
    "import random\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "# there are 2000 movie reviews\n",
    "\n",
    "# one liner \n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "            for category in movie_reviews.categories() \n",
    "            for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "# multiple lines \n",
    "documents = []\n",
    "for category in movie_reviews.categories ():\n",
    "    for fileid in movie_reviews.fileids(category):\n",
    "        documents.append((list(movie_reviews.words(fileid)), category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 77717), ('the', 76529), ('.', 65876), ('a', 38106), ('and', 35576), ('of', 34123), ('to', 31937), (\"'\", 30585), ('is', 25195), ('in', 21822), ('s', 18513), ('\"', 17612), ('it', 16107), ('that', 15924), ('-', 15595)]\n",
      "253\n",
      "<FreqDist with 39768 samples and 1583820 outcomes>\n"
     ]
    }
   ],
   "source": [
    "# Text classification continues\n",
    "\n",
    "# randomizing in preparation for training and testing \n",
    "random.shuffle(documents)\n",
    "\n",
    "#print(documents[0])\n",
    "\n",
    "# all words from all movies \n",
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())\n",
    "    \n",
    "# frequency distribution     \n",
    "# freq distribution is a dictionary of words and frequencies ordered by frequency \n",
    "all_words = nltk.FreqDist(all_words) \n",
    "print(all_words.most_common(15))\n",
    "print(all_words['stupid'])\n",
    "print(all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 12: Words as Features for Learning\n",
    "\n",
    "##### For our text classification, we have to find some way to \"describe\" bits of data, which are labeled as either positive or negative for machine learning training purposes. \n",
    "\n",
    "##### These descriptions are called \"features\" in machine learning. For our project, we're just going to simply classify each word within a positive or negative review as a \"feature\" of that review. \n",
    "\n",
    "##### Then, as we go on, we can train a classifier by showing it all of the features of positive and negative reviews (all the words), and let it try to figure out the more meaningful differences between a positive review and a negative review, by simply looking for common negative review words and common positive review words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get the top 3000 occurring words and the context in which they are used (either positive or negative) \n",
    "### train on this data and then determine if a review is positive or negative based on words used \n",
    "\n",
    "word_features = list(all_words.keys())[:3000]\n",
    "\n",
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        # dictionary of booleans determing whether or not top 3000 words \n",
    "        # across all movie reviews is in document/single review \n",
    "        features[w] = (w in words) \n",
    "    return features\n",
    "\n",
    "#print((find_features(movie_reviews.words('neg/cv000_29416.txt'))))\n",
    "\n",
    "featuresets = [ (find_features(review), category) for (review, category) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#featuresets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 13: Naives Bayes\n",
    "\n",
    "##### The algorithm of choice, at least at a basic level, for text analysis is often the Naive Bayes classifier. Part of the reason for this is that text data is almost always massive in size. The Naive Bayes algorithm is so simple that it can be used at scale very easily with minimal process requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Algo Accuracy: 77.0\n",
      "Most Informative Features\n",
      "                 idiotic = True              neg : pos    =     12.1 : 1.0\n",
      "                  annual = True              pos : neg    =     10.7 : 1.0\n",
      "               atrocious = True              neg : pos    =     10.5 : 1.0\n",
      "                   sucks = True              neg : pos    =      9.5 : 1.0\n",
      "                 frances = True              pos : neg    =      9.3 : 1.0\n",
      "           unimaginative = True              neg : pos    =      7.5 : 1.0\n",
      "                 cunning = True              pos : neg    =      7.0 : 1.0\n",
      "                  sexist = True              neg : pos    =      6.9 : 1.0\n",
      "             silverstone = True              neg : pos    =      6.9 : 1.0\n",
      "                  regard = True              pos : neg    =      6.9 : 1.0\n",
      "              schumacher = True              neg : pos    =      6.7 : 1.0\n",
      "                    mena = True              neg : pos    =      6.3 : 1.0\n",
      "                  shoddy = True              neg : pos    =      6.3 : 1.0\n",
      "                  suvari = True              neg : pos    =      6.3 : 1.0\n",
      "                 singers = True              pos : neg    =      6.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# categorizing as negative or positive sentiment \n",
    "\n",
    "# prepping training and testing sets \n",
    "training_set = featuresets[:1900]\n",
    "testing_set = featuresets[1900:]\n",
    "\n",
    "# NB algorithm: posterior = prior occurences x liklihood / evidence \n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "print(\"Naive Bayes Algo Accuracy:\", (nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 14: Save classifier with Pickle\n",
    "\n",
    "##### As you will likely find with any form of data analysis, there is going to be some sort of processing bottleneck, that you repeat over and over, often yielding the same object in Python memory. \n",
    "\n",
    "##### Examples of this might be loading a massive dataset into memory, some basic pre-processing of a static dataset, or, like in our case, the training of a classifier. \n",
    "\n",
    "##### In our case, we spend much time on training our classifier, and soon we may add more. It is a wise choice to go ahead and pickle the trained classifer. This way, we can load in the trained classifier in a matter of milliseconds, rather than waiting 3-5+ minutes for the classifier to be trained. \n",
    "\n",
    "##### To do this, we use the standard library's \"pickle\" module. What pickle does is serialize, or de-serialize, python objects. This could be lists, dictionaries, or even things like our trained classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Algo Accuracy: 77.0\n"
     ]
    }
   ],
   "source": [
    "# for saving a trained algorithm\n",
    "import pickle \n",
    "\n",
    "# saving trained classifier \n",
    "\n",
    "#save_classifier = open(\"naivebayes.pickle\",'wb')\n",
    "#pickle.dump(classifier, save_classifier)\n",
    "#save_classifier.close()\n",
    "\n",
    "# open to read \n",
    "classifier_f = open('naivebayes.pickle', 'rb')\n",
    "classifier = pickle.load(classifier_f)\n",
    "classifier_f.close()\n",
    "\n",
    "print(\"Naive Bayes Algo Accuracy:\", (nltk.classify.accuracy(classifier, testing_set))*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 15: Scikit-Learn Incorporation \n",
    "    \n",
    "##### Despite coming packed with some classifiers, NLTK is mainly a toolkit focused on natural language processing, and not machine learning specifically. \n",
    "\n",
    "##### A module that is focused on machine learning is scikit-learn, which is packed with a large array of machine learning algorithms which are optimized in C. \n",
    "\n",
    "##### Luckily NLTK has recognized this and comes packaged with a special classifier that wraps around scikit learn. In NLTK, this is: nltk.classify.scikitlearn, specifically the class:  SklearnClassifier is what we're interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "# support vector machines\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes Algo Accuracy: 77.0\n",
      "Bernoulli Naive Bayes Algo Accuracy: 77.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyterlab/conda/envs/python/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression Algo Accuracy: 80.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyterlab/conda/envs/python/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier Naive Bayes Algo Accuracy: 65.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyterlab/conda/envs/python/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC Algo Accuracy: 57.99999999999999\n",
      "Linear SVC Algo Accuracy: 81.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyterlab/conda/envs/python/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number SVC Algo Accuracy: 80.0\n"
     ]
    }
   ],
   "source": [
    "# converting sklearn classifier to nltk classifier using SklearnClassifier \n",
    "\n",
    "# dont forget to customize the parameters of the different classifying algos\n",
    "\n",
    "# Multinomial \n",
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(training_set)\n",
    "print(\"Multinomial Naive Bayes Algo Accuracy:\", (nltk.classify.accuracy(MNB_classifier, testing_set))*100)\n",
    "\n",
    "# gaussian\n",
    "#GNB_classifier = SklearnClassifier(GaussianNB())\n",
    "#GNB_classifier.train(training_set)\n",
    "#print(\"Gaussian Naive Bayes Algo Accuracy:\", (nltk.classify.accuracy(GNB_classifier, testing_set))*100)\n",
    "\n",
    "# bernoulli\n",
    "BNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "BNB_classifier.train(training_set)\n",
    "print(\"Bernoulli Naive Bayes Algo Accuracy:\", (nltk.classify.accuracy(BNB_classifier, testing_set))*100)\n",
    "\n",
    "# logistic\n",
    "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
    "LogisticRegression_classifier.train(training_set)\n",
    "print(\"LogisticRegression Algo Accuracy:\", (nltk.classify.accuracy(LogisticRegression_classifier, testing_set))*100)\n",
    "\n",
    "# SGDClassifier\n",
    "SGDClassifier_classifier = SklearnClassifier(SGDClassifier())\n",
    "SGDClassifier_classifier.train(training_set)\n",
    "print(\"SGDClassifier Algo Accuracy:\", (nltk.classify.accuracy(SGDClassifier_classifier, testing_set))*100)\n",
    "\n",
    "# SVC\n",
    "# much more innaccurate than others so removing\n",
    "#SVC_classifier = SklearnClassifier(SVC())\n",
    "#SVC_classifier.train(training_set)\n",
    "#print(\"SVC Algo Accuracy:\", (nltk.classify.accuracy(SVC_classifier, testing_set))*100)\n",
    "\n",
    "\n",
    "# Linear SVC\n",
    "LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_classifier.train(training_set)\n",
    "print(\"Linear SVC Algo Accuracy:\", (nltk.classify.accuracy(LinearSVC_classifier, testing_set))*100)\n",
    "\n",
    "# Linear SVC\n",
    "NuSVC_classifier = SklearnClassifier(NuSVC())\n",
    "NuSVC_classifier.train(training_set)\n",
    "print(\"Number SVC Algo Accuracy:\", (nltk.classify.accuracy(NuSVC_classifier, testing_set))*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 16: Combining Algos with a Vote\n",
    "\n",
    "##### Now that we have many classifiers, what if we created a new classifier, which combined the votes of all of the classifiers, and then classified the text whatever the majority vote was? \n",
    "\n",
    "##### Turns out, doing this is super easy. NLTK has considered this in advance, allowing us to inherit from their ClassifierI class from nltk.classify, which will give us the attributes of a classifier, yet allow us to write our own custom classifier code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voted Classifier Algo Accuracy: 76.0\n",
      "Classification: pos , Confidence %: 1.0\n",
      "Classification: pos , Confidence %: 1.0\n",
      "Classification: pos , Confidence %: 1.0\n",
      "Classification: pos , Confidence %: 1.0\n",
      "Classification: pos , Confidence %: 0.5714285714285714\n",
      "Classification: pos , Confidence %: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Voting system: each classifier gets one vote and category is chosen based on most votes\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode \n",
    "\n",
    "# building class \n",
    "\n",
    "class VoteClassifier (ClassifierI):\n",
    "    def __init__(self, *classifiers):\n",
    "        self.classifiers = classifiers\n",
    "    \n",
    "    def classify(self, features):\n",
    "        votes=[]\n",
    "        for c in self.classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        return mode(votes)\n",
    "    \n",
    "    def confidence(self, features):\n",
    "        votes = [] \n",
    "        for c in self.classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        choice_votes = votes.count(mode(votes))\n",
    "        conf = choice_votes/len(votes)\n",
    "        return conf\n",
    "    \n",
    "voted_classifier = VoteClassifier(classifier, MNB_classifier, BNB_classifier, \n",
    "                                  LogisticRegression_classifier, SGDClassifier_classifier,\n",
    "                                  LinearSVC_classifier, NuSVC_classifier)\n",
    "\n",
    "print(\"Voted Classifier Algo Accuracy:\", (nltk.classify.accuracy(voted_classifier, testing_set))*100)\n",
    "\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[0][0]), \", Confidence %:\", voted_classifier.confidence(testing_set[0][0]))\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[1][0]), \", Confidence %:\", voted_classifier.confidence(testing_set[1][0]))\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[2][0]), \", Confidence %:\", voted_classifier.confidence(testing_set[2][0]))\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[3][0]), \", Confidence %:\", voted_classifier.confidence(testing_set[3][0]))\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[4][0]), \", Confidence %:\", voted_classifier.confidence(testing_set[4][0]))\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[5][0]), \", Confidence %:\", voted_classifier.confidence(testing_set[5][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 17: Investigating Bias \n",
    "\n",
    "##### At this point in our project, we're interested in moving on to a real dataset, but we're concerned still about our volatility in accuracy. \n",
    "\n",
    "##### In this video, we peak into the classifiers to see if we have any bias leans towards positive or negative, and we wind up finding out that not only do we have a bias, we have a bug!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voted Classifier Algo Accuracy: 80.0\n"
     ]
    }
   ],
   "source": [
    "voted_classifier = VoteClassifier(MNB_classifier, BNB_classifier, \n",
    "                                  LogisticRegression_classifier,\n",
    "                                  LinearSVC_classifier, NuSVC_classifier)\n",
    "\n",
    "print(\"Voted Classifier Algo Accuracy:\", (nltk.classify.accuracy(voted_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 18: Better Training data \n",
    "\n",
    "##### After some consideration it became clear that a new dataset would solve a lot of problems. This tutorial covers employing a new dataset, and what is involved in this process. \n",
    "\n",
    "##### This time, we're using a movie reviews data set that contains much shorter movie reviews. \n",
    "\n",
    "##### You can get this data set from: http://pythonprogramming.net/static/d...\n",
    "\n",
    "##### This one yields us a far more reliable reading across the board, and is far more fitting for the tweets we intend to read from the Twitter API soon. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 19: Sentiment Analysis \n",
    " \n",
    "##### Now that we've got a more reliable classifier, we're ready to push forward. Here, we cover how we can convert our classifier training script to an actual sentiment analysis module. \n",
    "\n",
    "##### We pickle everything, and create a new sentiment function, which, with a parameter of \"Text\" will perform a classification and return the result. \n",
    "\n",
    "##### By pickling everything, we find that we can load this module in seconds, rather than the prior 3-5 minutes. After this, we're ready to apply this module to a live Twitter stream. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 20: Twitter Sentiment Analysis\n",
    "\n",
    "##### Finally, the moment we've all been waiting for and building up to. A live test! We've decided to employ this classifier to the live Twitter stream, using Twitter's API. \n",
    "\n",
    "##### We've already covered how to do live Twitter API streaming, if you missed it, you can catch up here: http://pythonprogramming.net/twitter-...After this, we output the findings to a text file, which we intend to graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 21: Graphing Live Twitter Sentiment\n",
    "    \n",
    "##### For a current conclusion to this series, we go ahead and graph our basic sentiment analysis results to a live Matplotlib graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
